<html>

<head>
    <title>ACRE: Abstract Causal REasoning Beyond Covariation</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
        content="About Machine Learning, Computer Vision, Life, Photo Gallery and Everything | Chi Zhang, Programmer, Machine Learning and Computer Vision">
    <meta name="keyword" content="Chi Zhang, 张驰, Blog, Internet, Machine Learning, Computer Vision">
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
        type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
        rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/css/project.css"">
</head>

<body>

<div id=" Banner">
    <div height="80" id="header" style="background-color:#FFFFFF; color: #FFFFFF">
        <center>
            <table width="1200" height="80" border="0">
                <tr>
                    <td halign="center">
                        <p class=un>ACRE: <u>A</u>bstract <u>C</u>ausal <u>RE</u>asoning Beyond Covariation</p>
                        <hr>
                    </td>
                </tr>
            </table>
        </center>
    </div>
    </div>

    <div id="main"
        style="padding-bottom:1em; padding-top: 2em; width: 70em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <center>
            <img src="/img/in-post/ACRE/model.jpeg" style="width: 100%;">
        </center>
        <br>
        <heading>
            Abstract
        </heading>
        <p>
            Causal induction, <i>i.e.</i>, identifying unobservable mechanisms that lead to the observable relations
            among variables, has played a pivotal role in modern scientific discovery, especially in scenarios with only
            sparse and limited data. Humans, even young toddlers, can induce causal relationships surprisingly well in
            various settings despite its notorious difficulty. However, in contrast to the commonplace trait of human
            cognition is the lack of a diagnostic benchmark to measure causal induction for modern Artificial
            Intelligence (AI) systems. Therefore, in this work, we introduce the Abstract Causal REasoning (ACRE)
            dataset for systematic evaluation of current vision systems in causal induction. Motivated by the stream of
            research on causal discovery in <i>Blicket</i> experiments, we query a visual reasoning system with the
            following four types of questions in either an independent scenario or an interventional scenario:
            <i>direct</i>, <i>indirect</i>, <i>screening-off</i>, and <i>backward-blocking</i>, intentionally going
            beyond the simple strategy of inducing causal relationships by covariation. By analyzing visual reasoning
            architectures on this testbed, we notice that pure neural models tend towards an associative strategy under
            their chance-level performance, whereas neuro-symbolic combinations struggle in backward-blocking reasoning.
            These deficiencies call for future research in models with a more comprehensive capability of causal
            induction.
        </p>
    </div>

    <div id="main"
        style="padding-bottom:0em; padding-top: 0em; width: 70em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <heading>
            Paper
        </heading>
        <p>
            <papertitle>ACRE: <u>A</u>bstract <u>C</u>ausal <u>RE</u>asoning Beyond Covariation</papertitle><br>
            Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu<br>
            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br>
            <a href="/attach/cvpr21zhang_acre.pdf">Paper</a> /
            <a href="/attach/cvpr21zhang_acre_supp.pdf">Supplementary</a> /
            <a href="https://github.com/WellyZhang/ACRE">Code</a> /
            <a href="/blog/2021/03/19/ACRE">Blog</a>
        </p>
        <p>
            <center>
                <a href="/attach/cvpr21zhang_acre.pdf"><img src="/img/project/acre_thumbnail.jpg"
                        style="width: 100%;" /></a>
            </center>
        </p>
    </div>

    <div id="main"
        style="padding-bottom:0em; padding-top: 2em; width: 70em; max-width: 70em; margin-left: auto; margin-right: auto; align-content: center">
        <heading>
            Team
        </heading>
        <div style="text-align: center; width: 100%; padding-top: 1em">
            <div style="display: inline-block; width: 180px;">
                <a href="http://wellyzhang.github.io"><img src="http://vcla.stat.ucla.edu/images/people/chiz.jpg" alt=""
                        style="border-radius: 50%; width:150px;">
                    <p>Chi Zhang</p>
                </a>
            </div>
            <div style="display: inline-block; width: 180px;">
                <a href="https://buzz-beater.github.io/"><img src="http://vcla.stat.ucla.edu/images/people/bxjia.jpg"
                        alt="" style="border-radius: 50%; width:150px;">
                    <p>Baoxiong Jia</p>
                </a>
            </div>
            <div style="display: inline-block; width: 180px;">
                <a href="https://mjedmonds.com/"><img src="https://vcla.stat.ucla.edu/images/people/medmonds.jpg" alt=""
                        style="border-radius: 50%; width:150px;">
                    <p>Mark Edmonds</p>
                </a>
            </div>
            <div style="display: inline-block; width: 180px;">
                <a href="http://www.stat.ucla.edu/~sczhu/"><img
                        src="http://vcla.stat.ucla.edu/images/people/Zhu_UCLA.JPG" alt=""
                        style="border-radius: 50%; width:150px;">
                    <p>Song-Chun Zhu</p>
                </a>
            </div>
            <div style="display: inline-block; width: 180px;">
                <a href="http://www.yzhu.io/"><img src="https://vcla.stat.ucla.edu/images/people/yzhu.jpg" alt=""
                        style="border-radius: 50%; width:150px;">
                    <p>Yixin Zhu</p>
                </a>
            </div>
        </div>
        <div style="text-align:center; width: 100%;">
            <div style="display: inline-block; width: 500px;">
                <p>UCLA Center for Vision, Cognition, Learning, and Autonomy</p>
            </div>
        </div>
    </div>

    <div id="main"
        style="padding-bottom:1em; padding-top: 0em; width: 80em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <heading id="dataset">
            Dataset
        </heading>
        <p>
            <center>
                <img src="/img/in-post/ACRE/example.jpeg" style="width: 100%;">
            </center>
            <ur>
                <li>Each ACRE problem consists of 10 panels: 6 for context and 4 for query.</li>
                <li>In queries, we ask a visual reasoning system to predict the state of the Blicket machine given the
                    objects in the queries.</li>
            </ur>
        </p>

        <p>
            In addition to the IID split, we create a compositionality split and a systematicity split.
            <ur>
                <li>Compositionality (Comp): we assign different shape-material-color combinations to the training and
                    test set similar to <a href="https://cs.stanford.edu/people/jcjohns/clevr/">CoGenT in CLEVR</a>.
                </li>
                <li>Systematicity (Sys): we vary the distribution of an activated Blicket detector in the context
                    panels, with the machine lighting up 3 times in the training set and 4 times during testing.</li>
            </ur>
        </p>

        <p>
            Download the dataset here and check our <a
                href="https://github.com/WellyZhang/ACRE/tree/main/src/helper">GitHub</a> for dataset organization.
            <ur>
                <li><a
                        href="https://drive.google.com/file/d/1P0WBnnjWolGsrATUQtx4ictiYlOGc-OT/view?usp=sharing">ACRE-IID</a>
                </li>
                <li><a
                        href="https://drive.google.com/file/d/1-LZMt08a1v-KSuaQTS1lqD6BCEw47LEY/view?usp=sharing">ACRE-Comp</a>
                </li>
                <li><a
                        href="https://drive.google.com/file/d/1Sn_tKbe6mMv7Tc_y6hJZnm7lSenjwIys/view?usp=sharing">ACRE-Sys</a>
                </li>
            </ur>
        </p>
    </div>

    <div id="main"
        style="padding-bottom:1em; padding-top: 0em; width: 80em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <heading>
            Code
        </heading>
        <p>
            View on <a href="https://github.com/WellyZhang/ACRE">GitHub</a>
        </p>
    </div>

    <div id="main"
        style="padding-bottom:1em; padding-top: 0em; width: 80em; max-width: 70em; margin-left: auto; margin-right: auto;">
        <heading>
            Bibtex
        </heading>
        <p class=bibtax>
            @inproceedings{zhang2021acre,
            <br>&emsp;title={ACRE: Abstract Causal REasoning Beyond Covariation},
            <br>&emsp;author={Zhang, Chi and Jia, Baoxiong and Edmonds, Mark and Zhu, Song-Chun and Zhu, Yixin},
            <br>&emsp;booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
            <br>&emsp;year={2021}
            <br>}
        </p>
    </div>

    </body>

</html>