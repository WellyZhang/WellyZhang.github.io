---
layout:     post
title:      "Release of SceneSet-50 and Code for Batch Automatic Transformation of 3D Models"
date:       2016-09-29
author:     "Welly Zhang"
header-img: "img/banner/post-banner-sceneset.jpg" 
catalog: true
tags: 
    - Computer Vision
---

> This blog post summarizes the interior scene dataset built when I was a research intern at VCLA@UCLA together with code for batch transformation of 3D models.

## 1 Introduction

Computer Vision has nowadays evolved to a stage where most practical solutions to classical vision problems rely heavily on data. One typical example in 2D that advances both the field of deep learning and computer vision has been the ImageNet dataset and the closely related Large Scale Visual Recognition Challenge, aka LSVRC, where researchers world-wide compete and compare performance of their algorithms to improve the upper limit of machines' ability to understand visual information from the real world. We believe that the same principle applies to 3D and more broadly, Virtual World from Virtual Reality or Augmented Reality, especially ones that robot agents reside so that resources required to train robots could be reduced to a much lower level, making it easier and cheaper. These in mind, we built the SceneSet-50 dataset with the hope that relevant research community could benefit from it on problems regarding scene understanding, 3D modelling, virtural reality and so on.

## 2 SceneSet-50

The SceneSet-50 dataset maintains **50** complete scenes, convering **5** cultural backgrounds -- American, Chinese, European, Japanese and Mediterranean and **5** room types -- bedroom, kitchen, laundry, living room and study, each of which has two instances, typically one traditional and one modern. Normally, every scene has more than **10** objects, some of which have even **interactive components**, such as doors, drawers, windows that open. We hope that the open-sourcing of the dataset will enable advanced research and progress in related computer vision directions, such as 3D reconstruction, virtual reality, scene understanding and especially robotics, where agents could be trained using the scenes without costly scene construction and catastrophic damage in reality.

To give you a feel of how the scenes look like:

![Bedroom](/img/in-post/SceneSet-50/Bedroom.jpg)
<small class="img-hint">Figure 1. A bedroom example.</small>

![Study](/img/in-post/SceneSet-50/Study.jpg)
<small class="img-hint">Figure 2. A study example.</small>

![Render](/img/in-post/SceneSet-50/Render.png)
<small class="img-hint">Figure 3. A rendered example.</small>

![Depth](/img/in-post/SceneSet-50/Depth.png)
<small class="img-hint">Figure 4. Depth feature map.</small>

We now welcome you to use our [dataset](https://drive.google.com/open?id=0B5TAsUHD4ncdVDUteEJsLVFUcm8) for computer science research and even fields outside computer science, such as cognitive science.

The directory contains the original models built by SketchUp and exported models in OBJ format, together with a guide on how to maintain and modify the dataset.

## 3 Batch Automatic Transformation of 3D Models

The batch automatic transformation script converts models of any file format into OBJ, aligns the center of the model's bounding box to the origin of the world coordinates, applies the transformation specified in a file and renames the texture files to make it easier for rendering engine.

The code reside [here](/attach/Auto3D-master.zip).

#### 3.1 Dependency

* Windows (currently working only under Windows)
* MeshLab v1.3.4 BETA

#### 3.2 Usage

To run the script, first add the meshlabserver to your environment path and then navigate to the directory where your models are stored. Assuming the transformation is prepared, you can simply run the script or import it and call function main as follows

```Python
import utils
utils.main("your_model_name", "template.mlx", "your_transformation")
```

An example is already provided in the repo: the \*_ml\*s are the script output.

The script will first convert your model into OBJ, read the transformation specified in the transformation file, fill the template to be fed to the meshlabserver, rename the textures, and update the references to the textures in the OBJ's mtl file.

Note that the transformed file should be stored in the same directory and a model-based script will be generated. The model-based script will also triangulate all the meshes in your model.

#### 3.3 Issues

If your model, after conversion to OBJ, does not have texture files, it's either that the model just doesn't have textures or that texture mapping is incorporated inside the .obj file by vertex coloring. Some software might not support this per-vertex color OBJ format.

So it's strongly encouraged to **use OBJ models** in the beginning so that the pipeline works well.

## References

\[1\] Wikipedia. [Automatic Number Plate Recognition](https://en.wikipedia.org/wiki/Automatic_number_plate_recognition).  
\[2\] Ross Girshick, Jeff Donahue, Trevor Darrell and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. *arXiv preprint*: 1311.2524.  
\[3\] Ross Girshick. Fast R-CNN. *arXiv preprint*: 1504.08083.  
\[4\] Shotton, Jamie, Andrew Blake, and Roberto Cipolla. Contour-based learning for object detection. *Tenth IEEE International Conference on Computer Vision (ICCV'05)* Volume 1. Vol. 1. IEEE, 2005.  
\[5\] Xu, Yong, et al. Contour-based recognition. *Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on*. IEEE, 2012.  
\[6\] Dalal, N., & Triggs, B. (2005, June). Histograms of oriented gradients for human detection. In *2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)* (Vol. 1, pp. 886-893). IEEE.  
\[7\] Lowe, David G. Distinctive image features from scale-invariant keypoints. *International journal of computer vision* 60.2 (2004): 91-110.  
\[8\] Bay, H., Tuytelaars, T., & Van Gool, L. (2006, May). Surf: Speeded up robust features. In *European conference on computer vision* (pp. 404-417). Springer Berlin Heidelberg.  
\[9\] Hilton Bristow and Simon Lucey. Why do linear SVMs trained on HOG features perform so well? *arXiv preprint*: 1406.2419.  
\[10\] Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. *IEEE transactions on pattern analysis and machine intelligence*, 32(9), 1627-1645.  
\[11\] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In *Advances in neural information processing systems* (pp. 1097-1105).  
\[12\] He, K., Zhang, X., Ren, S., & Sun, J. (2014, September). Spatial pyramid pooling in deep convolutional networks for visual recognition. In *European Conference on Computer Vision* (pp. 346-361). Springer International Publishing.  
\[13\] Girshick, R. (2015). Fast r-cnn. In *Proceedings of the IEEE International Conference on Computer Vision* (pp. 1440-1448).  
\[14\] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. In *Advances in neural information processing systems* (pp. 91-99).  
\[15\] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2015). You only look once: Unified, real-time object detection. *arXiv preprint* :1506.02640.  
\[16\] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., & Reed, S. (2015). SSD: Single Shot MultiBox Detector. *arXiv preprint*: 1512.02325.  
\[17\] Henriques, J. F., Carreira, J., Caseiro, R., & Batista, J. (2013). Beyond hard negative mining: Efficient detector learning via block-circulant decomposition. In *proceedings of the IEEE International Conference on Computer Vision* (pp. 2760-2767).  
\[18\] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., & LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. *arXiv preprint*: 1312.6229.  
\[19\] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (pp. 1-9).  
\[20\] He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep residual learning for image recognition. *arXiv preprint*: 1512.03385.  
\[21\] Wikipedia. [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function).   
\[22\] LeCun, Y. A., Bottou, L., Orr, G. B., & MÃ¼ller, K. R. (2012). Efficient backprop. In *Neural networks: Tricks of the trade* (pp. 9-48). Springer Berlin Heidelberg.  
